{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_translate_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwmGs5XyUxHL",
        "outputId": "456afa55-e04b-4452-ef73-bf5696d6e508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaZpWJzVV-mI"
      },
      "source": [
        "# import key libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "import plotly.express as px\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, RepeatVector\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHAEsliiWHyX",
        "outputId": "afbedab4-af00-43c9-fc90-f911640d99c2",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  \n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "data_path = 'machine_translation.txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a299c793-838e-40e3-aa3f-4179ec9165c1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a299c793-838e-40e3-aa3f-4179ec9165c1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving machine_translation.txt to machine_translation.txt\n",
            "User uploaded file \"machine_translation.txt\" with length 34494242 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07diE_TmWIHq"
      },
      "source": [
        "def read_file(data_path):\n",
        "  with open(data_path, 'rt') as file:\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aynBHNv9WNX5"
      },
      "source": [
        "def remove_punctuation(sentence):\n",
        "  remove_punc = [char for char in sentence if char not in string.punctuation]\n",
        "  combine_removed_punc = ''.join(remove_punc)\n",
        "  return combine_removed_punc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvOlCtbQWOrp"
      },
      "source": [
        "def to_lines(text):\n",
        "  sents = text.strip().split('\\n')\n",
        "  sents = [i.split('\\t') for i in sents]\n",
        "  return sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDE52HSPWSLZ"
      },
      "source": [
        "data = read_file('machine_translation.txt')\n",
        "french_n_eng = to_lines(data)\n",
        "french_n_eng = np.array(french_n_eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdLXWBkeWUGA"
      },
      "source": [
        "french_n_eng = french_n_eng[:25000,:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LEpsie5WV_Y"
      },
      "source": [
        "french_n_eng[:,0] = [s.translate(str.maketrans('','',string.punctuation)) for s in french_n_eng[:,0]]\n",
        "french_n_eng[:,1] = [s.translate(str.maketrans('','',string.punctuation)) for s in french_n_eng[:,1]]\n",
        "\n",
        "french_n_eng[:,0] = [s.lower() for s in french_n_eng[:,0]]\n",
        "french_n_eng[:,1] = [s.lower() for s in french_n_eng[:,1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ku19hx8xYLCX",
        "outputId": "5959d4da-d45a-4527-acab-47ebc4b40a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "french_n_eng"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['go', 'geh'],\n",
              "       ['hi', 'hallo'],\n",
              "       ['hi', 'grüß gott'],\n",
              "       ...,\n",
              "       ['we cant go there', 'wir können da nicht hingehen'],\n",
              "       ['we cant help tom', 'wir können tom nicht helfen'],\n",
              "       ['we cant help you', 'wir können dir nicht helfen']], dtype='<U537')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHWvqTvVY0dG"
      },
      "source": [
        "def max_length(lines):\n",
        "  return max(len(line.split()) for line in lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OXvWvouZd74"
      },
      "source": [
        "def tokenize_text(text):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(text)\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C1Wp78gaI1W",
        "outputId": "7e0251c0-52ea-42fa-8355-676bab80a242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "eng_tokenizer = tokenize_text(french_n_eng[:,0])\n",
        "french_tokenizer = tokenize_text(french_n_eng[:,1])\n",
        "\n",
        "english_vocab_length = len(eng_tokenizer.word_index) + 1\n",
        "french_vocab_length = len(french_tokenizer.word_index) + 1\n",
        "\n",
        "english_max_length = max_length(french_n_eng[:,0])\n",
        "french_max_length = max_length(french_n_eng[:,1])\n",
        "\n",
        "print(f'The total number of unique English words is: {english_vocab_length}')\n",
        "print(f'The total number of unique French words is: {french_vocab_length}')\n",
        "\n",
        "print(f'The maximum length of English sentences are: {english_max_length}')\n",
        "print(f'The maximum length of French sentences are: {french_max_length}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total number of unique English words is: 4247\n",
            "The total number of unique French words is: 6674\n",
            "The maximum length of English sentences are: 5\n",
            "The maximum length of French sentences are: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQE_IOfZbQSG"
      },
      "source": [
        "def sequence_encoding(tokenizer, length, sentences):\n",
        "  seq = tokenizer.texts_to_sequences(sentences)\n",
        "  seq = pad_sequences(seq, maxlen=length, padding='post')\n",
        "  return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QClRclLhdof6"
      },
      "source": [
        "def encode_output(sequences, vocab_size):\n",
        "  ylist = []\n",
        "  for sequence in sequences:\n",
        "    encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "    ylist.append(encoded)\n",
        "\n",
        "  y = np.array(ylist)\n",
        "  y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8eY-CcLeUUU"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(french_n_eng, test_size=0.25, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaqcYaO1fN9Z"
      },
      "source": [
        "X_train = sequence_encoding(french_tokenizer,french_max_length,train[:,1])\n",
        "y_train = sequence_encoding(eng_tokenizer,english_max_length,train[:,0])\n",
        "y_train = encode_output(y_train, english_vocab_length)\n",
        "\n",
        "X_test = sequence_encoding(french_tokenizer,french_max_length,test[:,1])\n",
        "y_test = sequence_encoding(eng_tokenizer,english_max_length,test[:,0])\n",
        "y_test = encode_output(y_test, english_vocab_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4Rdp9jzfPT7",
        "outputId": "cd2cbd82-14e7-4f21-f0eb-2413ac8e54e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "model = Sequential()\n",
        "model.add(Embedding(french_vocab_length, 512, input_length=french_max_length,mask_zero=True))\n",
        "model.add(LSTM(512))\n",
        "model.add(RepeatVector(english_max_length))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(english_vocab_length, activation='softmax')))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 10, 512)           3417088   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 5, 512)            2099200   \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 5, 4247)           2178711   \n",
            "=================================================================\n",
            "Total params: 9,794,199\n",
            "Trainable params: 9,794,199\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scRI3_XukL6M",
        "outputId": "0e6a477f-48bf-4e6e-9acc-211d054898bf",
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=64, validation_data=(X_test, y_test), callbacks=[checkpoint], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 4.1057\n",
            "Epoch 00001: val_loss improved from inf to 3.65273, saving model to model.h5\n",
            "293/293 [==============================] - 195s 667ms/step - loss: 4.1057 - val_loss: 3.6527\n",
            "Epoch 2/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 3.3597\n",
            "Epoch 00002: val_loss improved from 3.65273 to 3.25967, saving model to model.h5\n",
            "293/293 [==============================] - 194s 662ms/step - loss: 3.3597 - val_loss: 3.2597\n",
            "Epoch 3/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 2.9475\n",
            "Epoch 00003: val_loss improved from 3.25967 to 2.92614, saving model to model.h5\n",
            "293/293 [==============================] - 195s 664ms/step - loss: 2.9475 - val_loss: 2.9261\n",
            "Epoch 4/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 2.5413\n",
            "Epoch 00004: val_loss improved from 2.92614 to 2.63449, saving model to model.h5\n",
            "293/293 [==============================] - 191s 653ms/step - loss: 2.5413 - val_loss: 2.6345\n",
            "Epoch 5/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 2.1657\n",
            "Epoch 00005: val_loss improved from 2.63449 to 2.40862, saving model to model.h5\n",
            "293/293 [==============================] - 189s 647ms/step - loss: 2.1657 - val_loss: 2.4086\n",
            "Epoch 6/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 1.8422\n",
            "Epoch 00006: val_loss improved from 2.40862 to 2.23845, saving model to model.h5\n",
            "293/293 [==============================] - 191s 650ms/step - loss: 1.8422 - val_loss: 2.2384\n",
            "Epoch 7/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 1.5597\n",
            "Epoch 00007: val_loss improved from 2.23845 to 2.11281, saving model to model.h5\n",
            "293/293 [==============================] - 189s 646ms/step - loss: 1.5597 - val_loss: 2.1128\n",
            "Epoch 8/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 1.3116\n",
            "Epoch 00008: val_loss improved from 2.11281 to 2.01739, saving model to model.h5\n",
            "293/293 [==============================] - 190s 649ms/step - loss: 1.3116 - val_loss: 2.0174\n",
            "Epoch 9/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 1.0902\n",
            "Epoch 00009: val_loss improved from 2.01739 to 1.95054, saving model to model.h5\n",
            "293/293 [==============================] - 192s 656ms/step - loss: 1.0902 - val_loss: 1.9505\n",
            "Epoch 10/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.8987\n",
            "Epoch 00010: val_loss improved from 1.95054 to 1.89030, saving model to model.h5\n",
            "293/293 [==============================] - 190s 650ms/step - loss: 0.8987 - val_loss: 1.8903\n",
            "Epoch 11/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.7373\n",
            "Epoch 00011: val_loss improved from 1.89030 to 1.85850, saving model to model.h5\n",
            "293/293 [==============================] - 192s 656ms/step - loss: 0.7373 - val_loss: 1.8585\n",
            "Epoch 12/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.6037\n",
            "Epoch 00012: val_loss improved from 1.85850 to 1.83733, saving model to model.h5\n",
            "293/293 [==============================] - 192s 654ms/step - loss: 0.6037 - val_loss: 1.8373\n",
            "Epoch 13/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.4926\n",
            "Epoch 00013: val_loss did not improve from 1.83733\n",
            "293/293 [==============================] - 188s 640ms/step - loss: 0.4926 - val_loss: 1.8419\n",
            "Epoch 14/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.4068\n",
            "Epoch 00014: val_loss improved from 1.83733 to 1.81913, saving model to model.h5\n",
            "293/293 [==============================] - 188s 642ms/step - loss: 0.4068 - val_loss: 1.8191\n",
            "Epoch 15/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.3376\n",
            "Epoch 00015: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 189s 647ms/step - loss: 0.3376 - val_loss: 1.8317\n",
            "Epoch 16/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.2859\n",
            "Epoch 00016: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 187s 639ms/step - loss: 0.2859 - val_loss: 1.8422\n",
            "Epoch 17/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.2460\n",
            "Epoch 00017: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 188s 643ms/step - loss: 0.2460 - val_loss: 1.8616\n",
            "Epoch 18/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.2151\n",
            "Epoch 00018: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 190s 648ms/step - loss: 0.2151 - val_loss: 1.8678\n",
            "Epoch 19/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1923\n",
            "Epoch 00019: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 193s 658ms/step - loss: 0.1923 - val_loss: 1.8925\n",
            "Epoch 20/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1743\n",
            "Epoch 00020: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 191s 653ms/step - loss: 0.1743 - val_loss: 1.8902\n",
            "Epoch 21/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1650\n",
            "Epoch 00021: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 189s 645ms/step - loss: 0.1650 - val_loss: 1.9076\n",
            "Epoch 22/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1525\n",
            "Epoch 00022: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 195s 665ms/step - loss: 0.1525 - val_loss: 1.9301\n",
            "Epoch 23/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1463\n",
            "Epoch 00023: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 191s 653ms/step - loss: 0.1463 - val_loss: 1.9438\n",
            "Epoch 24/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1414\n",
            "Epoch 00024: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 192s 656ms/step - loss: 0.1414 - val_loss: 1.9574\n",
            "Epoch 25/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1363\n",
            "Epoch 00025: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 193s 658ms/step - loss: 0.1363 - val_loss: 1.9731\n",
            "Epoch 26/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1332\n",
            "Epoch 00026: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 188s 643ms/step - loss: 0.1332 - val_loss: 1.9889\n",
            "Epoch 27/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1309\n",
            "Epoch 00027: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 187s 639ms/step - loss: 0.1309 - val_loss: 1.9991\n",
            "Epoch 28/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1275\n",
            "Epoch 00028: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 186s 637ms/step - loss: 0.1275 - val_loss: 2.0017\n",
            "Epoch 29/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1249\n",
            "Epoch 00029: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 185s 632ms/step - loss: 0.1249 - val_loss: 2.0259\n",
            "Epoch 30/30\n",
            "293/293 [==============================] - ETA: 0s - loss: 0.1235\n",
            "Epoch 00030: val_loss did not improve from 1.81913\n",
            "293/293 [==============================] - 186s 636ms/step - loss: 0.1235 - val_loss: 2.0310\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f9c446eb6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shmaLg3gkloD",
        "colab": {
          "background_save": true
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwLzT338t9W8",
        "colab": {
          "background_save": true
        }
      },
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfmpn3OjuF8X",
        "colab": {
          "background_save": true
        }
      },
      "source": [
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [np.argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP42hJ_buJYT",
        "colab": {
          "background_save": true
        }
      },
      "source": [
        "# evaluate the skill of the model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_IN0lWHuNgC",
        "outputId": "7d5a17dc-8564-45ff-a518-0df4807e5278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, X_train, train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[tom wird verlieren], target=[tom will lose], predicted=[tom lose]\n",
            "src=[sie lachen], target=[theyre laughing], predicted=[theyre laughing]\n",
            "src=[wir lächelten], target=[we smiled], predicted=[we smiled]\n",
            "src=[tom grimassierte], target=[tom grimaced], predicted=[tom grimaced]\n",
            "src=[ich mag pferde], target=[i like horses], predicted=[i like horses]\n",
            "src=[wir nennen ihn tom], target=[we call him tom], predicted=[we call him tom]\n",
            "src=[kinder sind grausam], target=[kids are cruel], predicted=[kids are cruel]\n",
            "src=[ich verehre sie], target=[i adore you], predicted=[i adore you]\n",
            "src=[heute ist sonnabend], target=[today is saturday], predicted=[today is saturday]\n",
            "src=[ihr macht wohl witze], target=[youre joking], predicted=[youre joking]\n",
            "BLEU-1: 0.906205\n",
            "BLEU-2: 0.864896\n",
            "BLEU-3: 0.815807\n",
            "BLEU-4: 0.601791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqy8jo7KuUea",
        "outputId": "4452b495-c397-4548-b9fd-6e9ac3116531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, X_test, test)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test\n",
            "src=[tom war gelangweilt], target=[tom was bored], predicted=[tom was bored]\n",
            "src=[tom ist schwer verletzt], target=[tom is badly hurt], predicted=[tom died hurt]\n",
            "src=[tom hat dienst], target=[tom is on duty], predicted=[tom have kids]\n",
            "src=[tom ist vorbereitet], target=[tom is prepared], predicted=[tom is stunned]\n",
            "src=[tom will geld], target=[tom wants money], predicted=[tom wants money]\n",
            "src=[kühe fressen gras], target=[cows eat grass], predicted=[how a your numb]\n",
            "src=[ich werde tom ausrufen lassen], target=[ill page tom], predicted=[ill go tom tom]\n",
            "src=[zeigen sie uns wo es langgeht], target=[show us the way], predicted=[lets us the]\n",
            "src=[komm sofort hierher], target=[come at once], predicted=[come back once]\n",
            "src=[tom ist eingeschritten], target=[tom intervened], predicted=[tom is fanatical]\n",
            "BLEU-1: 0.597706\n",
            "BLEU-2: 0.477394\n",
            "BLEU-3: 0.408898\n",
            "BLEU-4: 0.246018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZdpHNBT6IAH",
        "outputId": "51704d94-df59-42df-c914-cf9e3951d3f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "preds = model.predict_classes(X_test.reshape((X_test.shape[0],X_test.shape[1])))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-30-ff8518dbc17d>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAlpHmcrujF3"
      },
      "source": [
        "preds_text = []\n",
        "for i in preds:\n",
        "       temp = []\n",
        "       for j in range(len(i)):\n",
        "            t = word_for_id(i[j], eng_tokenizer)\n",
        "            if j > 0:\n",
        "                if (t == word_for_id(i[j-1], eng_tokenizer)) or (t == None):\n",
        "                     temp.append('')\n",
        "                else:\n",
        "                     temp.append(t)\n",
        "            else:\n",
        "                   if(t == None):\n",
        "                          temp.append('')\n",
        "                   else:\n",
        "                          temp.append(t) \n",
        "\n",
        "       preds_text.append(' '.join(temp))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5859TOf5PG9",
        "outputId": "22d772db-8d88-456d-ce23-ba4fc5436ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})\n",
        "pred_df.sample(15)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1432</th>\n",
              "      <td>beat it</td>\n",
              "      <td>go away</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1684</th>\n",
              "      <td>can you show me</td>\n",
              "      <td>can you see me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2280</th>\n",
              "      <td>were you jealous</td>\n",
              "      <td>were you jealous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2404</th>\n",
              "      <td>this is a pun</td>\n",
              "      <td>thats a fir tree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>678</th>\n",
              "      <td>its secret</td>\n",
              "      <td>this is secret</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5369</th>\n",
              "      <td>im not certain</td>\n",
              "      <td>i not sure</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5288</th>\n",
              "      <td>tom is a player</td>\n",
              "      <td>tom is a bad</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2683</th>\n",
              "      <td>its about time</td>\n",
              "      <td>its so wrong</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4761</th>\n",
              "      <td>i need support</td>\n",
              "      <td>i need internet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>tom wasnt crazy</td>\n",
              "      <td>tom wasnt crazy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6101</th>\n",
              "      <td>im over thirty</td>\n",
              "      <td>im thirty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2266</th>\n",
              "      <td>this is my father</td>\n",
              "      <td>this is my room</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3707</th>\n",
              "      <td>that isnt fair</td>\n",
              "      <td>this unfair</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3266</th>\n",
              "      <td>it looks so real</td>\n",
              "      <td>it looks so</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3105</th>\n",
              "      <td>it was my dream</td>\n",
              "      <td>it was a dream</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 actual           predicted\n",
              "1432            beat it          go away   \n",
              "1684    can you show me     can you see me \n",
              "2280   were you jealous  were you jealous  \n",
              "2404      this is a pun   thats a fir tree \n",
              "678          its secret    this is secret  \n",
              "5369     im not certain        i not sure  \n",
              "5288    tom is a player       tom is a bad \n",
              "2683     its about time      its so wrong  \n",
              "4761     i need support   i need internet  \n",
              "675     tom wasnt crazy   tom wasnt crazy  \n",
              "6101     im over thirty        im thirty   \n",
              "2266  this is my father    this is my room \n",
              "3707     that isnt fair      this unfair   \n",
              "3266   it looks so real       it looks so  \n",
              "3105    it was my dream     it was a dream "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERKNAOSJ6MT7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}